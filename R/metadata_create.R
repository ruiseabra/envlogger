# >> ----
# metadata_create_fn("test")
metadata_create_fn <- function(paths) {
  paths %>% fs::path_ext_remove() %>% stringr::str_c("_meta.csv")
}


# >> ----
# update = TRUE; path = env_example("inst/extdata/humid/2025-05-06/humidsc01a-0482_AC00_F27D_01-20250506_095113.csv")$rep; meta_row = tibble::tibble(step = "", path_data = path, path_meta = metadata_create_fn(path), new_vals = list(list(id = "test", tdiff = 10, purge_temp_min = -60, split_time_gap = TRUE, bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED")))
# x <- metadata_create_file_single(meta_row, update); fs::file_delete(x)
metadata_create_file_single <- function(
    meta_row,
    update = FALSE
) {
  # this function can only handle one file at a time
  if (nrow(meta_row) != 1) cli::cli_abort(c("x" = "'meta_row' must point to a single file"))

  # prepare metadata file template
  template <- template_metadata %>%
    dplyr::select(-rep_env, -rep_other)

  # populate template if valid new_vals are available
  new_vals <- meta_row$new_vals[[1]]
  if (!is.null(new_vals)) {
    if (is.list(new_vals)) {
      for (nm in names(new_vals)) {
        pos <- which(template$field == nm)
        if (length(pos)) {
          template$new_val[[pos]] <- new_vals[[nm]]
        }
      }
    }
  }

  # if the target metadata file already exists, either...
  if (file.exists(meta_row$path_meta)) {
    if (update) {
      # read previous values and append to the new template
      ## only where new values have not been supplied
      old_data <- readr::read_csv(
        meta_row$path_meta,
        skip = 4,
        progress = FALSE,
        show_col_types = FALSE) %>%
        dplyr::filter(!is.na(new_val))
      if (nrow(old_data)) {
        for (i in seq_along(old_data$field)) {
          f   <- old_data$field[[i]]
          pos <- which(template$field == f)
          if (is.na(template$new_val[[pos]])) template$new_val[[pos]] <- old_data$new_val[[i]]
        }
      }
    } else {
      # or stop if update is disabled
      cli::cli_abort(c("!" = "the target metadata file already exists"))
    }
  }

  # write metadata file
  ## header
  readr::write_lines(
    c(
      glue::glue("metadata,-------,----,{fs::path_file(meta_row$path_data)}"),
      glue::glue("pkg vsn: {packageVersion('envlogger')},-------,----,only edit 'new_val' --> leave header unchanged"),
      glue::glue("created: {Sys.Date()},-------,----,unused lines can be left as NA or deleted"),
      "---------------------,-------,----,-----------------------------------------"
    ),
    file = meta_row$path_meta
  )
  ## fields and new_vals
  readr::write_csv(
    template,
    file = meta_row$path_meta,
    append = TRUE,
    col_names = TRUE
  )

  # return
  meta_row$path_meta
}


# >>>> ----
#' Create a metadata file
#'
#' @description
#' Create an `EnvLogger` metadata file. These files can be used to provide additional information about the matching data file, as well as to store correction details necessary for the successful import of data using [read_env()].
#'
#' @param path The path to a single data report file (additional details below).
#' @param new_vals A list with values to be added to the metadata file about to be created. If not supplied, a default template file with all fields set to `NA` is generated. Elements in `new_vals` must have the exact same name of the field they correspond to, and the values provided must be single element vectors convertible to character. Elements with names not present in the `field` column of the template metadata file are silently disregarded (additional details below).
#' @param update If a metadata file is already present, should the execution stop with an error or should the file be updated with the new values (new values take precedence over the values in the original file, but values in the previous file for fields not covered in the current `new_vals` are left unchanged).
#'
#' @return
#' The path of the metadata file that has been created. In addition, an EnvLogger metadata file is created. The file path to the new file is generated by appending `_meta.csv` to the end of the `path` provided. The file created is a `CSV` file with four columns:
#' * `field` - the name of the metadata field
#' * `new_val` - the new value to be assigned
#' * `type` - the format to which `new_val` will be converted
#' * `info` - details about the meaning and constraints of each `field`
#'
#' @section Why use metadata files:
#' Large databases of environmental data often include files with errors, inconsistencies and formatting issues. While users can correct such files manually and definitively, such an approach removes the ability to traceback the changes made, sometimes making it impossible to revert those changes later on. This can be a source of many headaches, especially when data has been collected by multiple individuals, using different devices and from distinct locations - and it only gets tougher the larger the data collection network. One should never underestimate the likelihood of new information coming to light years later that implies a reorganization of the database, in which case reverting some of the changes made earlier may be necessary. A way to mitigate this effect is to store corrections in a different file. In this approach, the original file remains intact, and it is the job of the matching metadata file to store the changes and corrections that need to be applied in order for the original data to make perfect sense within the database. Inspecting the metadata file is quick and changes introduced are explicit, traceable and easily revertible. Furthermore, since the metadata file only contains essential fields, the disk space metadata files take up is negligible. That is why the [envlogger-package] provides tools to create and read metadata files, and why our team advocates for the use of such files instead of implementing changes/corrections permanently into the original files.
#'
#' @section Corrections to EnvLogger files:
#' While EnvLogger report files should have no formatting issues, the data they contain may still require corrections. In this case, there are two use scenarios. In the first, the user is aware of a specific issue and wants to pro-actively address it. To do that, the user can generate a template metadata file (if the `new_vals` argument isn't provided, all fields are set to `NA`) and then manually edit the relevant fields. As long as the metadata file is kept in the same folder in which the associated file is stored, the changes it encodes will be reflected when [read_env()] is used to import the data. The second situation corresponds to when [read_env()] itself identifies certain 'standard' issues for which it can provide a correction. In that case, if the user allows, [read_env()] can automatically generate metadata files that will already include the correction parameters necessary. For example, if [read_env()] detects that a file includes timestamps obviously wrong (such as '1970-01-01 00:00'), it will generate a metadata file where the field `purge_time_bef` is set to '2000-01-01 00:00'. This way, the next time [read_env()] is executed, all entries in the offending file with a timestamp before '2000-01-01 00:00' will be discarded, and data import can resume without issue. Note that an argument could be made that the correction could just be silently applied without the need to generate a metadata file, making the process simpler and reducing clutter. However, such an approach would leave the user blindsided, unaware of changes that may introduce biases in the dataset. Since environmental datasets are often used for high-stakes research, conservation and management, such a level of ambiguity is unacceptable, and therefore the more explicit and traceable approach is used instead.
#'
#' @section Data files not conforming to the EnvLogger ecosystem:
#' The [envlogger-package] package is built around the EnvLogger ecosystem - devices, smartphone apps and the files they generate. However, our team is aware that many users have data generated using other devices that they want to easily integrate into their environmental datasets. While not all data formats can be accommodated, some flexibility can be gained through the use of metadata files. In this case, these files are used to pass on to [read_env()] certain basic bits of information essential for their correct reading, namely how many lines of header to skip, the format used to store time and a few other parameters. To this effect, there is no automation provided, and the user must generate one metadata file at a time for each of the non-conforming files and manually edit the relevant fields. While this can be laborious, it only needs to be done once for each file. Afterwards, the files are read normally as if the were standard EnvLogger files.
#'
#' @section Supported files:
#' * EnvLogger data report files with data (not just the header)
#' * EnvLogger log files
#' * EnvLogger metadata files (created with [metadata_create_file()])
#' * Other data report files originating from devices other than EnvLoggers, as long as they are accompanied by an EnvLogger metadata file with values set for fields required for their correct interpretation
#'
#' @section Non-EnvLogger data report files:
#' To ensure a high likelihood of success reading data generated outside of the EnvLogger ecosystem using [read_env()], not only must each non-Envlogger data report file be accompanied by a metadata file created with [metadata_create_file()], they must also obey certain simple structure rules, namely:
#' * be a text file, typically `CSV` or `TXT`
#' * if present, any contextual information must all be at the head of the file
#' * data must be stored by rows
#' * the data section may include column names, but that isn't mandatory
#' * each row of data must be in the form `date_time, temperature`
#' * each row of data must include only two columns
#' * temperature must be represented only as a number (not including the unit)
#' * the last line of the file is the last row of data
#' Then, certain information must be provided via the metadata files, namely:
#' * skip = number of lines to skip to get to the first row of actual data (not the column names, if present; can be zero or any number of lines)
#' * time_format = the format of date and time (not needed if date_time is in the standard format `2025-01-01 10:30:00`, corresponding to "%Y-%m-%d %H:%M:%S; check [strptime()] for more details about time formats)
#' * sep_dec_comma = set to `TRUE` if the temperature decimal separator is a comma (not needed if the decimal separator is the standard dot `"."`)
#' * additional fields such as `id`, `serial`, `tdiff` and others may be needed to ensure that all features available in [read_env()] are leveraged, but importing can proceed without them
#'
#' @section Issues - Unsupported files:
#' * Any file not listed in the section **Supported files**.
#' * These files can be left in place without interfering with data import.
#' * If you want to silence this warning, either remove affected files from your database or add their paths to the `avoid_pattern` arg when calling [read_env()] (such paths are available after a call to [read_env()] using `$files_with_issues`).
#'
#' @section Issues - Quality issues:
#' * Data report files that have not passed the quality checks.
#' * Issues flagged include the presence of temperature values outside the limits provided, timestamps outside the limits provided, `NA`s on data or timestamps, or timestamps not unidirectional, constant and without gaps.
#' * Files that do not pass quality checks are not included in the output.
#' * Therefore, action is needed to ensure that the affected files are imported.
#' * To fix this, either manually edit the files addressing the issues identified or (preferably) generate a metadata file with [metadata_create_file()] for each affected file and edit that metadata file to include the necessary information to resolve the issues. Afterwards, call [read_env()] again to implement the fixes.
#' * Alternatively, set the `auto_generate_fixes` and `auto_implement_fixes` args in [read_env()] to `TRUE`, as this will allow for the automatic creation of metadata files addressing the issues identified and the immediate implementation of those fixes.
#'
#' @section Issues - Full overlaps:
#' * Files that fully overlap other files are redundant.
#' * Affected files can be left in place without interfering with data import.
#' * However, if you want to silence this warning, either remove affected files from your database or add their paths to the `avoid_pattern` arg when calling [read_env()].
#'
#' @section Issues - Partial overlaps SERIALs:
#' * Files listed overlap other files for the same serial by more than the stipulated `overlap_max_mins` arg provided in the call to [read_env()].
#' * Affected files are still joined by serial - all data in older files is kept, while overlapping data in newer files is discarded.
#' * These overlaps are often benign, but consider double checking.
#' * To address this, either review the affected files and manually correct the overlapping issues permanently or generate a metadata file with [metadata_create_file()] and adjust `purge_time_bef`.
#'
#' @section Issues - Partial overlaps IDs:
#' * The typical origin of this issue is linked to a change in the logger device used at a monitoring site.
#' * Affected data is still joined by id - all data in older serial is kept, while overlapping data in newer serial is discarded.
#' * These overlaps are often benign, but consider double checking.
#' * To address this, either review the affected files and manually correct the overlapping issues permanently or generate a metadata file with [metadata_create_file()] and adjust `purge_time_bef`.
#' * ATTENTION! In certain situations, set a value for `purge_time_aft` instead.
#'
#' @section Real deployment date:
#' It is common practice among certain users to start a logger's mission before actually heading to the field and installing the logger. Depending how this is done, such practice can result in data being collected before it is actually meaningful. In other circumstances, if a logger has been deployed in a non-permanent way, the user may have recovered the logger, brought it back to the lab and only then downloaded the data. In this second situation, the logger's memory will include readings collected after the real deployment that are also not meaningful for any analysis.
#' To exclude such excess data, crate a metadata file and set the following parameters :
#' * set `purge_time_bef` to the date when the affected logger was actually installed; any data before this date will be discarded by [read_env()]
#' * set `purge_time_aft` to the date when the affected logger was actually recovered; any data after this date will be discarded by [read_env()]
#'
#' @section Timezone issues:
#' Potential situations:
#' * The smartphone used to program the loggers was set to the wrong timezone
#' * The smartphone used to retrieve the data was set to the wrong timezone
#'
#' Consequences:
#' * Data timestamps may be shifted from UTC
#' * Time diff may include timezone diff
#'
#' Edit fields:
#' * `offset_time` = shift timestamps by `x` seconds (pos or neg)
#' * `offset_diff` = shift time diff by `x` seconds (pos or neg)
#'
#' @seealso [read_env()], [env_ls()], [env_example()]
#' @export
#'
#' @examples
#' path <- env_example("2024-01-12/ptzzymh02a-04CB_CC00_1507_0C-20240112_083030.csv")$report
#' path_meta <- paste0(substr(path, 1, nchar(path) - 4), "_meta.csv")
#'
#' # creating an empty template metadata file
#' file.exists(path_meta) # metadata file doesn't exist yet
#' void <- metadata_create_file(path)
#' file.exists(path_meta) # metadata has been created
#' read.csv(path_meta, skip = 4) # all fields are set to NA; the user can adjust any necessary values
#' void <- env_example(delete_new_metadata_files = TRUE) # delete the new file
#'
#' # passing on values during the call to 'metadata_create_file()'
#' new_vals = list(
#'    id = "test",
#'    tdiff = 10,
#'    purge_temp_min = -60,
#'    split_time_gap = TRUE,
#'    bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED")
#'
#' void <- metadata_create_file(path, new_vals, update = TRUE)
#' df_meta <- read.csv(path_meta, skip = 4)
#' df_meta[!is.na(df_meta$new_val),] # metadata file now includes non-NA values
#' # note that the field 'bad_field' isn't present
#' void <- env_example(delete_new_metadata_files = TRUE) # delete the new file
# --- #
# update = TRUE; new_vals = list(id = "test", tdiff = 10, purge_temp_min = -60, split_time_gap = TRUE, bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED"); paths <- env_example("inst/extdata/humid/2025-05-06/humidsc01a-0482_AC00_F27D_01-20250506_095113.csv")$rep
# x <- metadata_create_file(paths, new_vals, update); fs::file_delete(x)
metadata_create_file <- function(
    path,
    new_vals = NULL,
    update   = FALSE
) {
  # if path is file paths, arrange as a tibble
  if (is.character(path)) {
    meta_rows <- tibble::tibble(
      step = "",
      path_data = path,
      path_meta = metadata_create_fn(path),
      new_vals  = list(new_vals)
    )
  }
  # if path points to a tibble of with the same structure as new_metadata, just use it directly
  if (tibble::is_tibble(path)) {
    if (identical(
      c(colnames(path), "run2") %>% unique() %>% sort(),
      colnames(append_issues()) %>% sort()
    ))  {
      meta_rows <- path
    }
  }

  # stop if update is FALSE and any of the metadata files to be created already exists
  if (!update & any(fs::file_exists(meta_rows$path_meta))) cli::cli_abort(c("!" = "one or more target metadata files already exist"))

  # otherwise, create each of the metadata files
  created <- vector(mode = "character", length = nrow(meta_rows))
  for (i in seq_along(meta_rows$path_meta)) {
    created[i] <- metadata_create_file_single(meta_rows[i,], update = update)
  }

  # return
  created
}
