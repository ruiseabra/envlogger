# >> ----
# append_issues(paths = "path", step = "step", issue = "issue", new_vals = list(id = "id"))
append_issues <- function(
    paths = NULL,
    step,
    issue,
    new_vals = NULL,
    df       = NULL
) {
  if (is.null(df)) {
    df <- tibble::tibble(
      step      = "delete",
      path_data = "delete",
      path_meta = "delete",
      issue     = "delete",
      new_vals  = list(list(delete = "delete")),
      run2      = TRUE,
      fixed     = FALSE
    ) %>%
      dplyr::slice(-1)
  }

  if (tibble::is_tibble(paths)) paths <- paths %>% dplyr::pull(path)
  if (length(paths)) {
    df <- dplyr::bind_rows(
      df,
      tibble::tibble(
        step      = step,
        path_data = paths,
        path_meta = if (is.null(new_vals)) "" else metadata_fn(paths),
        issue     = issue,
        new_vals  = new_vals,
        run2      = TRUE,
        fixed     = FALSE
      )
    )
  }
  df
}


# >> ----
# metadata_fn("test")
metadata_fn <- function(paths) {
  paths %>% fs::path_ext_remove() %>% stringr::str_c("_meta.csv")
}


# >> ----
# update = TRUE; path = env_example("inst/extdata/humid/2025-05-06/humidsc01a-0482_AC00_F27D_01-20250506_095113.csv")$rep; meta_row = tibble::tibble(step = "", path_data = path, path_meta = metadata_fn(path), new_vals = list(list(id = "test", tdiff = 10, purge_temp_min = -60, split_time_gap = TRUE, bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED")))
# x <- create_metadata_file_single(meta_row, update); fs::file_delete(x)
create_metadata_file_single <- function(
    meta_row,
    update = FALSE
) {
  # this function can only handle one file at a time
  if (nrow(meta_row) != 1) cli::cli_abort(c("x" = "'meta_row' must point to a single file"))

  # prepare metadata file template
  template <- template_metadata %>%
    dplyr::select(-rep_env, -rep_other)

  # populate template if valid new_vals are available
  new_vals <- meta_row$new_vals[[1]]
  if (!is.null(new_vals)) {
    if (is.list(new_vals)) {
      for (nm in names(new_vals)) {
        pos <- which(template$field == nm)
        if (length(pos)) {
          template$new_val[[pos]] <- new_vals[[nm]]
        }
      }
    }
  }

  # if the target metadata file already exists, either...
  if (file.exists(meta_row$path_meta)) {
    if (update) {
      # read previous values and append to the new template
      ## only where new values have not been supplied
      old_data <- readr::read_csv(
        meta_row$path_meta,
        skip = 4,
        progress = FALSE,
        show_col_types = FALSE) %>%
        dplyr::filter(!is.na(new_val))
      if (nrow(old_data)) {
        for (i in seq_along(old_data$field)) {
          f   <- old_data$field[[i]]
          pos <- which(template$field == f)
          if (is.na(template$new_val[[pos]])) template$new_val[[pos]] <- old_data$new_val[[i]]
        }
      }
    } else {
      # or stop if update is disabled
      cli::cli_abort(c("!" = "the target metadata file already exists"))
    }
  }

  # write metadata file
  ## header
  readr::write_lines(
    c(
      glue::glue("metadata,-------,----,{fs::path_file(meta_row$path_data)}"),
      glue::glue("pkg vsn: {packageVersion('envlogger')},-------,----,only edit 'new_val' --> leave header unchanged"),
      glue::glue("created: {Sys.Date()},-------,----,unused lines can be left as NA or deleted"),
      "---------------------,-------,----,-----------------------------------------"
    ),
    file = meta_row$path_meta
  )
  ## fields and new_vals
  readr::write_csv(template, file = meta_row$path_meta, append = TRUE, col_names = TRUE)

  # return
  meta_row$path_meta
}


# >>>> ----
#' Create a metadata file
#'
#' @description
#' Create an `EnvLogger` metadata file. These files can be used to provide additional information about the matching data file, as well as to store correction details necessary for the successful import of data using [read_env()].
#'
#' @param path The path to a single data report file (additional details below).
#' @param new_vals A list with values to be added to the metadata file about to be created. If not supplied, a default template file with all fields set to `NA` is generated. Elements in `new_vals` must have the exact same name of the field they correspond to, and the values provided must be single element vectors convertible to character. Elements with names not present in the `field` column of the template metadata file are silently disregarded (additional details below).
#' @param update If a metadata file is already present, should the execution stop with an error or should the file be updated with the new values (new values take precedence over the values in the original file, but values in the previous file for fields not covered in the current `new_vals` are left unchanged).
#'
#' @return
#' The path of the metadata file that has been created. In addition, an EnvLogger metadata file is created. The file path to the new file is generated by appending `_meta.csv` to the end of the `path` provided. The file created is a `CSV` file with four columns:
#' * `field` - the name of the metadata field
#' * `new_val` - the new value to be assigned
#' * `type` - the format to which `new_val` will be converted
#' * `info` - details about the meaning and constraints of each `field`
#'
#' @section Why use metadata files:
#' Large databases of environmental data often include files with errors, inconsistencies and formatting issues. While users can correct such files manually and definitively, such an approach removes the ability to traceback the changes made, sometimes making it impossible to revert those changes later on. This can be a source of many headaches, especially when data has been collected by multiple individuals, using different devices and from distinct locations - and it only gets tougher the larger the data collection network. One should never underestimate the likelihood of new information coming to light years later that implies a reorganization of the database, in which case reverting some of the changes made earlier may be necessary. A way to mitigate this effect is to store corrections in a different file. In this approach, the original file remains intact, and it is the job of the matching metadata file to store the changes and corrections that need to be applied in order for the original data to make perfect sense within the database. Inspecting the metadata file is quick and changes introduced are explicit, traceable and easily revertible. Furthermore, since the metadata file only contains essential fields, the disk space metadata files take up is negligeble. That is why the [envlogger-package] provides tools to create and read metadata files, and why our team advocates for the use of such files instead of implementing changes/corrections permanently into the original files.
#'
#' @section Corrections to EnvLogger files:
#' While EnvLogger report files should have no formatting issues, the data they contain may still require corrections. In this case, there are two use scenarios. In the first, the user is aware of a specific issue and wants to pro-actively address it. To do that, the user can generate a template metadata file (if the `new_vals` argument isn't provided, all fields are set to `NA`) and then manually edit the relevant fields. As long as the metadata file is kept in the same folder in which the associated file is stored, the changes it encodes will be reflected when [read_env()] is used to import the data. The second situation corresponds to when [read_env()] itself identifies certain 'standard' issues for which it can provide a correction. In that case, if the user allows, [read_env()] can automatically generate metadata files that will already include the correction parameters necessary. For example, if [read_env()] detects that a file includes timestamps obviously wrong (such as '1970-01-01 00:00'), it will generate a metadata file where the field `purge_time_bef` is set to '2000-01-01 00:00'. This way, the next time [read_env()] is executed, all entries in the offending file with a timestamp before '2000-01-01 00:00' will be discarded, and data import can resume without issue. Note that an argument could be made that the correction could just be silently applied without the need to generate a metadata file, making the process simpler and reducing clutter. However, such an approach would leave the user blindsided, unaware of changes that may introduce biases in the dataset. Since environmental datasets are often used for high-stakes research, conservation and management, such a level of ambiguity is unacceptable, and therefore the more explicit and traceable approach is used instead.
#'
#' @section Data files not conforming to the EnvLogger ecosystem:
#' The [envlogger-package] package is built around the EnvLogger ecosystem - devices, smartphone apps and the files they generate. However, our team is aware that many users have data generated using other devices that they want to easily integrate into their environmental datasets. While not all data formats can be accommodated, some flexibility can be gained through the use of metadata files. In this case, these files are used to pass on to [read_env()] certain basic bits of information essential for their correct reading, namely how many lines of header to skip, the format used to store time and a few other parameters. To this effect, there is no automation provided, and the user must generate one metadata file at a time for each of the non-conforming files and manually edit the relevant fields. While this can be laborious, it only needs to be done once for each file. Afterwards, the files are read normally as if the were standard EnvLogger files.
#'
#' @section Supported files:
#' * EnvLogger data report files with data (not just the header)
#' * EnvLogger log files
#' * EnvLogger metadata files (created with [create_metadata_file()])
#' * Other data report files originating from devices other than EnvLoggers, as long as they are accompanied by an EnvLogger metadata file with values set for fields required for their correct interpretation
#'
#' @section Non-EnvLogger data report files:
#' To ensure a high likelihood of success reading data generated outside of the EnvLogger ecosystem using [read_env()], not only must each non-Envlogger data report file be accompanied by a metadata file created with [create_metadata_file()], they must also obey certain simple structure rules, namely:
#' * be a text file, typically `CSV` or `TXT`
#' * if present, any contextual information must all be at the head of the file
#' * data must be stored by rows
#' * the data section may include column names, but that isn't mandatory
#' * each row of data must be in the form `date_time, temperature`
#' * each row of data must include only two columns
#' * temperature must be represented only as a number (not including the unit)
#' * the last line of the file is the last row of data
#' Then, certain information must be provided via the metadata files, namely:
#' * skip = number of lines to skip to get to the first row of actual data (not the column names, if present; can be zero or any number of lines)
#' * time_format = the format of date and time (not needed if date_time is in the standard format `2025-01-01 10:30:00`, corresponding to "%Y-%m-%d %H:%M:%S; check [strptime()] for more details about time formats)
#' * sep_dec_comma = set to `TRUE` if the temperature decimal separator is a comma (not needed if the decimal sparator is the standard dot `"."`)
#' * additional fields such as `id`, `serial`, `tdiff` and others may be needed to ensure that all features available in [read_env()] are leveraged, but importing can proceed without them
#'
#' @section Issues - Unsupported files:
#' * Any file not listed in the section **Supported files**.
#' * These files can be left in place without interfering with data import.
#' * If you want to silence this warning, either remove affected files from your database or add their paths to the `avoid_pattern` arg when calling [read_env()] (such paths are available after a call to [read_env()] using `$files_with_issues`).
#'
#' @section Issues - Quality issues:
#' * Data report files that have not passed the quality checks.
#' * Issues flagged include the presence of temperature values outside the limits provided, timestamps outside the limits provided, `NA`s on data or timestamps, or timestamps not unidirectional, constant and without gaps.
#' * Files that do not pass quality checks are not included in the output.
#' * Therefore, action is needed to ensure that the affected files are imported.
#' * To fix this, either manually edit the files addressing the issues identified or (preferably) generate a metadata file with [create_metadata_file()] for each affected file and edit that metadata file to include the necessary information to resolve the issues. Afterwards, call [read_env()] again to implement the fixes.
#' * Alternatively, set the `auto_generate_fixes` and `auto_implement_fixes` args in [read_env()] to `TRUE`, as this will allow for the automatic creation of metadata files addressing the issues identified and the immediate implementation of those fixes.
#'
#' @section Issues - Full overlaps:
#' * Files that fully overlap other files are redundant.
#' * Affected files can be left in place without interfering with data import.
#' * However, if you want to silence this warning, either remove affected files from your database or add their paths to the `avoid_pattern` arg when calling [read_env()].
#'
#' @section Issues - Partial overlaps SERIALs:
#' * Files listed overlap other files for the same serial by more than the stipulated `overlap_max_mins` arg provided in the call to [read_env()].
#' * Affected files are still joined by serial - all data in older files is kept, while overlapping data in newer files is discarded.
#' * These overlaps are often benign, but consider double checking.
#' * To address this, either review the affected files and manually correct the overlapping issues permanently or generate a metadata file with [create_metadata_file()] and adjust `purge_time_bef`.
#'
#' @section Issues - Partial overlaps IDs:
#' * The typical origin of this issue is linked to a change in the logger device used at a monitoring site.
#' * Affected data is still joined by id - all data in older serial is kept, while overlapping data in newer serial is discarded.
#' * These overlaps are often benign, but consider double checking.
#' * To address this, either review the affected files and manually correct the overlapping issues permanently or generate a metadata file with [create_metadata_file()] and adjust `purge_time_bef`.
#' * ATTENTION! In certain situations, set a value for `purge_time_aft` instead.
#'
#' @section Real deployment date:
#' It is common practice among certain users to start a logger's mission before actually heading to the field and installing the logger. Depending how this is done, such practice can result in data being collected before it is actually meaningful. In other circumstances, if a logger has been deployed in a non-permanent way, the user may have recovered the logger, brought it back to the lab and only then downloaded the data. In this second situation, the logger's memory will include readings collected after the real deployment that are also not meaningful for any analysis.
#' To exclude such excess data, crate a metadata file and set the following parameters :
#' * set `purge_time_bef` to the date when the affected logger was actually installed; any data before this date will be discarded by [read_env()]
#' * set `purge_time_aft` to the date when the affected logger was actually recovered; any data after this date will be discarded by [read_env()]
#'
#' @section Timezone issues:
#' Potential situations:
#' * The smartphone used to program the loggers was set to the wrong timezone
#' * The smartphone used to retrieve the data was set to the wrong timezone
#'
#' Consequences:
#' * Data timestamps may be shifted from UTC
#' * Time diff may include timezone diff
#'
#' Edit fields:
#' * `offset_time` = shift timestamps by `x` seconds (pos or neg)
#' * `offset_diff` = shift time diff by `x` seconds (pos or neg)
#'
#'
#' @export
#'
#' @examples
#' path <- env_example("2024-01-12/ptzzymh02a-04CB_CC00_1507_0C-20240112_083030.csv")$report
#' path_meta <- paste0(substr(path, 1, nchar(path) - 4), "_meta.csv")
#'
#' # creating an empty template metadata file
#' file.exists(path_meta) # metadata file doesn't exist yet
#' void <- create_metadata_file(path)
#' file.exists(path_meta) # metadata has been created
#' read.csv(path_meta, skip = 4) # all fields are set to NA; the user can adjust any necessary values
#' void <- env_example(delete_new_metadata_files = TRUE) # delete the new file
#'
#' # passing on values during the call to 'create_metadata_file()'
#' new_vals = list(
#'    id = "test",
#'    tdiff = 10,
#'    purge_temp_min = -60,
#'    split_time_gap = TRUE,
#'    bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED")
#'
#' void <- create_metadata_file(path, new_vals, update = TRUE)
#' df_meta <- read.csv(path_meta, skip = 4)
#' df_meta[!is.na(df_meta$new_val),] # metadata file now includes non-NA values
#' # note that the field 'bad_field' isn't present
#' void <- env_example(delete_new_metadata_files = TRUE) # delete the new file
# --- #
# update = TRUE; new_vals = list(id = "test", tdiff = 10, purge_temp_min = -60, split_time_gap = TRUE, bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED"); paths <- env_example("inst/extdata/humid/2025-05-06/humidsc01a-0482_AC00_F27D_01-20250506_095113.csv")$rep
# x <- create_metadata_file(paths, new_vals, update); fs::file_delete(x)
create_metadata_file <- function(
    path,
    new_vals = NULL,
    update   = FALSE
) {
  # if path is file paths, arrange as a tibble
  if (is.character(path)) {
    meta_rows <- tibble::tibble(
      step = "",
      path_data = path,
      path_meta = metadata_fn(path),
      new_vals  = list(new_vals)
    )
  }
  # if path points to a tibble of with the same structure as new_metadata, just use it directly
  if (tibble::is_tibble(path)) {
    if (identical(
      c(colnames(path), "run2") %>% unique() %>% sort(),
      colnames(append_issues()) %>% sort()
    ))  {
      meta_rows <- path
    }
  }

  # stop if update is FALSE and any of the metadata files to be created already exists
  if (!update & any(fs::file_exists(meta_rows$path_meta))) cli::cli_abort(c("!" = "one or more target metadata files already exist"))

  # otherwise, create each of the metadata files
  created <- vector(mode = "character", length = nrow(meta_rows))
  for (i in seq_along(meta_rows$path_meta)) {
    created[i] <- create_metadata_file_single(meta_rows[i,], update = update)
  }

  # return
  created
}


# >>>> ----
#' List EnvLogger files
#'
#' @description
#' Search `paths` for EnvLogger files (EnvLogger reports, logfiles and metadata files).
#'
#' @param paths A character vector of one or more paths. May include folders, which are searched recursively.
#' @param avoid_pattern A character vector of one or more patterns. File paths matching to `avoid_pattern` are dropped.
#' @param list_unsupported Should unsupported files be included in the output?
#'
#' @return
#' A tibble with nine columns, including the file paths (`path`), two columns to indicate the type of files encountered, and six columns to facilitate quick filtering of specific file categories.
#'
#' In more detail, the two columns that indicate the type of files encountered are `$int` (numeric) and `$chr` (character), and their values are as follows:
#' * `1` and `report`      if an EnvLogger report or a valid report with a different format and an associated metadata file
#' * `2` and `log`         if an EnvLogger logfile
#' * `3` and `metadata`    if an EnvLogger metadata file
#' * `0` and `unsupported` if none of the three
#'
#' The remaining columns are logical vectors for quick filtering, namely:
#' * `rep`       - path points to a valid report of any kind?
#' * `rep_env`   - path points to an EnvLogger report?
#' * `rep_other` - path points to a report with a different format?
#' * `has_met`   - path points to a report with an associated metadata file?
#' * `log`       - path points to an EnvLogger logfile?
#' * `met`       - path points to an EnvLogger metadata file?
#'
#' @section Details:
#' `env_ls()` determines if the paths provided point to EnvLogger reports, logfiles, metadata files or none of the three.
#' This is done by checking for the presence of certain strings in the header of each file (such as "www.electricblue.eu, Portugal").
#' As a result, reports generated using the the earliest versions of the EnvLogger_Viewer app may not be recognized.
#' Also, if the structure of a file has been changed (e.g., by opening and saving in Excel), it may no longer be recognized.
#'
#' @section Non-EnvLogger data reports:
#' Otherwise unsupported files that are accompanied by an associated metadata file are assumed to be valid report files created by devices outside of the EnvLogger ecosystem, and will therefore be listed as "valid reports" (but this is no guarantee that they will be successfully read by downstream functions).
#'
#' @export
#'
#' @examples
#' paths <- env_example()
#' env_ls(paths)
# --- #
# paths <- env_example(); avoid_pattern = NULL; list_unsupported = TRUE
# env_ls(paths, avoid_pattern, list_unsupported)
env_ls <- function(
    paths,
    avoid_pattern = NULL,
    list_unsupported = FALSE
) {
  # support for env_example lists
  nms <- c("unsupported", "metadata", "log", "report")
  if (is.list(paths)) if (all(names(paths) %in% nms)) paths <- purrr::list_c(paths)

  # search within directories provided
  file_paths <- tibble::tibble(
    path  = fs::path_abs(paths),
    isdir = file.info(path)$isdir
  ) %>%
    split(.$isdir)

  # files are already accessible
  if (!is.null(file_paths$`FALSE`)) file_paths$`FALSE` <- file_paths$`FALSE`$path
  # folders must be recursively searched for files
  if (!is.null(file_paths$`TRUE`))  file_paths$`TRUE`  <- file_paths$`TRUE`$path %>% fs::dir_ls(recurse = TRUE, type = "file")

  # merge
  file_paths <- file_paths %>%
    purrr::list_c() %>%
    unique() %>%
    stringr::str_to_lower()

  # ...if any paths available
  if (length(file_paths)) {
    # drop files matching avoid_pattern
    file_paths <- avoid_pattern %>%
      c("env_archive") %>%
      purrr::map(~stringr::str_subset(file_paths, .x, negate = TRUE)) %>%
      purrr::reduce(intersect)

    # check file types and status
    file_paths <- tibble::tibble(
      path = file_paths,

      # detect files for which there's an associated metadata file
      ## when that is the case for unsupported files, they are in turn assumed to be valid reports that do not conform to the envlogger format, but that must be imported anyway through parsing
      has_met = file_paths %>%
        fs::path_ext_remove() %>%
        stringr::str_c("_meta.csv") %>%
        magrittr::is_in(file_paths)
    ) %>%

      dplyr::mutate(
        # read header
        l = purrr::map_chr(path, ~.x %>%
                             readr::read_lines(
                               n_max = 20,
                               progress = FALSE) %>%
                             stringr::str_to_lower() %>%
                             stringr::str_flatten_comma()),

        # find env rep strings
        is_env = purrr::map_lgl(l, ~ c(
          "www.electricblue.eu",
          "envlogger"
        ) %>%
          stringr::str_detect(.x, .) %>%
          any()),

        # find log strings
        is_log = purrr::map_lgl(l, ~ c(
          "tap logger",
          "data downloaded",
          "mission running",
          "waiting to start"
        ) %>%
          stringr::str_detect(.x, .) %>%
          any()),

        # find metadata strings
        is_met = purrr::map_lgl(l, ~ c(
          "metadata"
        ) %>%
          stringr::str_detect(.x, .) %>%
          any()),

        # compute numeric representation
        int = 0,
        int = dplyr::if_else(is_env, 1, int),
        int = dplyr::if_else(!is_env & has_met, 2, int),
        int = dplyr::if_else(is_log, 3, int),
        int = dplyr::if_else(is_met, 4, int),

        # compute string representation
        chr = dplyr::case_when(
          int == 0 ~ "unsupported",
          int == 1 ~ "report",
          int == 2 ~ "report",
          int == 3 ~ "log",
          int == 4 ~ "metadata"
        ),

        # logical is TRUE if corresponds to any valid Envlogger file
        lgl = int != 0,

        # additional columns to facilitate subsetting
        # csv  = fs::path_ext(path) == "csv",
        rep       = int == 1 | int == 2,
        rep_env   = int == 1,
        rep_other = int == 2,
        log = int == 3,
        met = int == 4,

        int = dplyr::if_else(int > 1, int - 1, int)
      ) %>%
      dplyr::select(-l, -dplyr::starts_with("is_"))
  } else {
    # no valid files found
    cli::cli_abort(glue::glue("'paths' doesn't point to ", cli::col_red("ANY"), " file"))
  }

  # tidy
  file_paths <- file_paths %>%
    dplyr::relocate(has_met, .after = "rep_other") %>%
    dplyr::arrange(int, path)

  # stop/warn if none of the paths point to EnvLogger files
  msg <- c(
    "x" = stringr::str_c("'paths' doesn't point to ", cli::col_red("ANY"), " EnvLogger file or folder(s) containing EnvLogger files"),
    "i" = "supported files: EnvLogger reports, logfiles and metadata files"
  )

  if (all(!file_paths$lgl)) {
    if (list_unsupported) cli::cli_warn(msg) else cli::cli_abort(msg)
  }

  # return
  if (!list_unsupported) file_paths <- dplyr::filter(file_paths, lgl)
  dplyr::select(file_paths, -lgl)
}


# >>>> ----
#' Get paths to example files
#'
#' @description
#' The [envlogger-package] comes bundled with several sample files in its inst/extdata directory. `env_example()` provides a quick shortcut to access the paths to these sample files.
#'
#' @param pattern Patterns to select one or more example files. `pattern` is vectorized, so all values supplied are used. If `NULL` (default) all example files are returned.
#' @param just_dir If `TRUE` only folder paths are returned.
#' @param delete_new_metadata_files When executing the examples provided for some functions in the [envlogger-package], new metadata files may be created, which in turn may limit the scope of other examples. If set to `TRUE`, any metadata file that isn't part of the original example files bundle is deleted, allowing for a clean slate.
#'
#' @return
#' A named list with the full path to one or more example files, grouped by EnvLogger file type. If `just_dir = TRUE`, a character vector with folder paths.
#'
#' @section Details:
#' The sample files provided consist of EnvLogger reports, logfiles and metadata files concerning several years of data from a few rocky shores in northern Portugal and Norway.
#' The content in some of the files has been modified to introduce issues that exemplify some of the package's quality control features.
#' Files include:
#' * "normal" data, just temperature - `nozzz`, `ptzzw`, and `ptzzy`
#' * "normal" data, just temperature, different format - `notenv`
#' * "normal" data, temperature and pressure - `press`
#' * "normal" data, temperature and humidity - `humid`
#' * "abnormal" data, with several quality issues - `issue`
#'
#' @export
#'
#' @examples
#' # Get the file paths of all example files
#' env_example()
#'
#' # Get the file paths to a target serial
#' env_example("04CB_CC00_1507_0C")
#'
#' # Get the file paths to any example files matching a search string
#' env_example("20250114")
#'
#' # Get only logfiles
#' env_example()$log
#'
#' # Get only folder paths
#' env_example(just_dir = TRUE)
#'
#' # 'env_example()' is vectorized, meaning that multiple search strings can be used at once
#' env_example(c("ptzzy", "nozzz"))
# --- #
# pattern  = "."; just_dir = FALSE
# env_example(pattern, just_dir)
env_example <- function(
    pattern  = ".",
    just_dir = FALSE,
    delete_new_metadata_files = FALSE
) {
  folder <- system.file("extdata", package = "envlogger")
  files  <- folder %>%
    env_ls(list_unsupported = TRUE) %>%
    dplyr::filter(
      purrr::map_lgl(path, ~.x %>%
                       stringr::str_detect(stringr::str_to_lower(pattern)) %>%
                       any())
    )

  if(delete_new_metadata_files) {
    to_drop <- tibble::tibble(
      path = files %>%
        dplyr::filter(met) %>%
        dplyr::pull(path)
    ) %>%
      dplyr::mutate(
        fn = fs::path_file(path),
        keep = purrr::map_lgl(
          fn,
          ~stringr::str_detect(example_files_to_keep, .x) %>% any()
        )
      ) %>%
      dplyr::filter(!keep)

    if (nrow(to_drop)) fs::file_delete(to_drop$path)

    # fetch file list again
    files  <- folder %>%
      env_ls(list_unsupported = TRUE) %>%
      dplyr::filter(
        purrr::map_lgl(path, ~.x %>%
                         stringr::str_detect(stringr::str_to_lower(pattern)) %>%
                         any())
      )
  }

  if (just_dir) {
    files$path %>% fs::path_dir() %>% unique()
  } else {
    split(files$path, files$chr)
  }
}


# >> ----
# header <- env_example("ptzzw")$rep[[1]] %>% readr::read_csv(progress = FALSE, n_max = 21, col_names = c("field", "val"), show_col_types = FALSE); path = "test"
# fix_header(header, path)
fix_header <- function(header, path) {
  header <- header %>%
    dplyr::mutate(dplyr::across(dplyr::everything(), stringr::str_to_lower))

  # fix fields that were named differently in earlier versions
  header$field[header$field == "time diff (sec)"] <- "time diff [logger-smartphone](sec)"
  header$field[header$field == "time diff [logger-smartphone](s)"] <- "time diff [logger-smartphone](sec)"

  # fix fields that were missing in earlier versions
  f <- "custom name"
  if (!any(header$field == f)) header <- tibble::add_row(header, field = f, val = "")
  f <- "file name"
  if (!any(header$field == f)) header <- tibble::add_row(header, field = f, val = path)

  fields <- c("device", "device name")
  for (f in fields) {
    if (!any(header$field == f))
      header <- tibble::add_row(header, field = f, val = "missing")
  }

  # fix values that were formatted improperly in earlier versions
  if (is.na(env_header_val(header, "envlogger viewer version")$dbl))
    header$val[header$field == "envlogger viewer version"] <- 0

  # return
  header
}


# >> ----
# headers <- env_example("ptzzw")$rep[[1]] %>% readr::read_csv(progress = FALSE, n_max = 21, col_names = c("field", "val"), show_col_types = FALSE); field_pattern = "device"
# env_header_val(headers, field_pattern)
env_header_val <- function(
    headers,
    field_pattern
) {
  if (tibble::is_tibble(headers)) headers <- list(headers)
  vals <- tibble::tibble(
    chr = headers %>%
      purrr::map_chr(~.x %>%
                       dplyr::filter(field == field_pattern) %>%
                       dplyr::pull(val)),
    l   = purrr::map_dbl(chr, length),
    dbl = purrr::map_dbl(chr, ~suppressWarnings(readr::parse_number(.x))),
    txt = purrr::map_chr(chr, ~suppressWarnings(stringr::str_remove_all(.x, "[-0-9.]"))) %>% stringr::str_trim()
  )
  if (any(vals$l != 1)) cli::cli_abort(c("x" = "'field_pattern' must match no more and no less than 1 time"))
  # return
  dplyr::select(vals, chr, dbl, txt)
}


# >> ----
# t1 = read_env_all(env_example("ptzzw")$rep[[1]], read_data = TRUE)$rep$data[[1]]$t; t2 = read_env_all(env_example("gap")$rep[[1]], read_data = TRUE)$rep$data[[1]]$t; buffer_secs = 10
# t_check(t1, buffer_secs); t_check(t2, buffer_secs)
t_check <- function(t, buffer_secs = 10) {
  if (tibble::is_tibble(t)) t <- t$t
  d <- diff(t)
  units(d) <- "secs"

  d <- as.numeric(d)
  d_med <- median(d)
  d_var <- abs(d_med - d)

  all(d > 0) & !any(d_var > buffer_secs) # timestamps are unidirectional, constant and without gaps
}


# >> ----
# msg = c("v" = stringr::str_c(cli::col_green("bef"), "ore")); bullet = "x"; vec = stringr::str_c("af", cli::col_red("ter")); col = "yellow"
# bullets(msg, bullet, vec, col) %>% cli::cli_bullets()
bullets <- function(msg, bullet = " ", vec, col = NULL) {
  COLS <- c("red", "blue", "yellow", "green", "white", "black", "cyan", "grey", "magenta", "sliver", "white", "none")
  if (!is.null(col)) if (col %in% COLS) vec <- eval(parse(text = stringr::str_c("cli::col_", col)))(vec)
  names(vec) <- rep_len(bullet, length(vec))
  msg <- c(msg, vec)
  msg
}

# >> ----
# paths = env_example("issue")$rep; paths = stringr::str_remove(paths, fs::path_common(paths)); issues = as.character(seq_along(paths)^2); path_width = max(stringr::str_length(paths)) + 3
# path_issues_bullets(paths, issues, path_width) %>% cli::cli_bullets()
path_issues_bullets <- function(paths, issues, fn_width) {
  paths  <- paths %>%
    stringr::str_c(" ") %>%
    cli::col_blue() %>%
    stringr::str_pad(
      width = fn_width + 10,
      side = "right",
      pad = ".",
      use_width = FALSE)

  issues <- purrr::map_vec(issues, ~stringr::str_c(cli::col_red(stringr::str_split_1(.x, ",")), collapse = ","))

  stringr::str_c(paths, " (", issues, ")")
}

# >> ----
# dat = NULL; fun = function(x) {list(rep = mean(x), msg_bullets = "ok")}; section_nm = "test"; use_dat = FALSE; x = 1:10
# run_section(dat = dat, use_dat = use_dat, fun = fun, section_nm = section_nm, x = x)
run_section <- function(
    dat = NULL,
    use_dat = TRUE,
    fun,
    section_nm = "",
    show_progress = TRUE,
    ...
) {
  dat <- if (use_dat) fun(dat, ...) else fun(...)
  if (show_progress) dat$msg_bullets %>% dplyr::last() %>% cli::cli_bullets()
  dat
}


