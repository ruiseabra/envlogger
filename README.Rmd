---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# envlogger

<!-- badges: start -->
<!-- badges: end -->

Functions to handle EnvLogger data, from reading to processing and plotting.  
If data conforms to the CCTBON naming scheme, additional functionalities are available.

## Installation

You can install the development version of **envlogger** from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("ruiseabra/envlogger")
```

## Load envlogger
```{r load}
library(envlogger)
```

## Example data
The package **envlogger** comes bundled with example data, including modified files that exemplify issues that the package's functions can handle automatically.
Use this dataset to get familiarized with the package's capabilities.
```{r example_data}
# files included
lapply(env_example(), basename)
```

## Read data
Use `read_env()` to import EnvLogger data into R. Supported files include:
* EnvLogger data report files with data (not just the header)
* EnvLogger log files
* EnvLogger metadata files (created with `create_metadata_file()`)
* Other data report files originating from devices other than EnvLoggers, as long as they are accompanied by an EnvLogger metadata file with values set for fields required for their correct interpretation
```{r read1}
# here we use the package's example data, specifically 
#  targeting data from site "nozzz" (a rocky shore in Norway)
env <- read_env(
  env_example("nozzz"), 
  show_progress = FALSE, 
  show_warnings = FALSE, 
  log_summary = TRUE)
```

Logfile data contains a record of all interactions with EnvLoggers. It is useful mainly for tracing back experimental errors or recovering a forgotten logger password.
```{r read2}
env$log
```

Reports comprise the data collected by each logger. Depending on the params used, data may be shown separately for each data file, or joined by serial or even by id.
```{r read3}
env$report
```

## Handling issues and unsupported files

As datasets of environmental data grow larger, the occurrence of data report files that contain errors or follow different field formats becomes inevitable. Most errors tend to be benign, meaning that they hardly impact data quality and mainly just cause programming issues that prevent their easy and automatic interpretation. Examples of such errors include the presence of gaps, `NA`s in the time or data fields, or spurious temperatures outside realistic ranges (-80 Â°C), which are indicative of momentary/permanent logger faults. In such cases, **envlogger** can identify the issue and suggest a correction. Suggested corrections are only implemented automatically upon explicit command from the user, as doing otherwise could introduce changes to the data to which the user would be unaware (very unlikely, but unacceptable). On the other hand, there are instances when errors are complex and **envlogger** won't be able to identify what exactly is wrong. A third situation involves the importing of data that wasn't generated by EnvLoggers, and that therefore follows a different data organization format.

To handle most of the abovementioned situations, the **envlogger** package relies on metadata files. Metadata files are companion files - one per data file with issues - that contain information that ensures that the data file they are paired with can be properly imported. For benign errors, those files can be generated automatically by calling `read_env(..., auto_generate_fixes = TRUE, auto_implement_fixes = TRUE)`. In that case, if issues are detected that the **envlogger** knows how to handle, new metadata files are created. Subsequent data imports using `read_env(apply_fixes = TRUE)` will result in those metadata files being read and the information they contain being used to correct all issues. This means that once a metadata file is created, so long that the metadata file is kept together with the its companion file, the issue(s) it targets will cease to be reported during any subsequent data imports - it is "fixed". For files with different data layout formats, a metadata file must be created manually with a call to `metadata_create_file()` and, at a minimum, values for `id`, `skip`, `time_format`, and `sep_dec_comma` need to be set.

While relying on companion metadata files may seem a cumbersome approach, it ensures that the original data remains unaltered, preserving data integrity and guaranteeing that fixes can be reassessed - and even reversed - at any moment, even many years after. This aspect is so crucial, that it alone justifies the use of metadatada files.

```{r issues1}
issues <- read_env(env_example("issue"))
```

As can be seen, the function `read_env()` provides detailed feedback about issues present in the data (set `show_warnings` to `FALSE` to disable that). Crucially, the output also contains the paths to the files with issues, and that can be used to make the creation of the metadata files easier.  

```{r issues2}
# create a metadata file
new_file <- metadata_create_file(
  issues$files_with_issues[1], 
  new_vals = list(id = "new_id")
  )

# metadata_create_file() outputs the path to the new file created
file.exists(new_file)
```
```{r issues3, include = FALSE}
void <- env_example(delete_new_metadata_files = TRUE)
```

## Parsing information contained in logger ids

If used in a thoughtful way, logger custom names (hereafter referred as `ids`) can greatly reduce the difficulty of organizing the data collected with EnvLoggers, especially for large networks comprising data from dozens or even hundreds of loggers. While logger ids are restricted to 10 characters, a lot of important details can be coded into those ids. 

The example data included has been collected using some of the ~3000 EnvLoggers of the CCTBON network (<www.coastalwarming.com/cctbon>), and therefore the custom names are set according to the CCTBON naming scheme. Specifically, CCTBON names are composed of:
* 5 characters for site name (2 characters for country, 3 for site); THIS MUST BE UNIQUE WITHIN THE NETWORK
* 2 characters for microhabitat type (in this case, one character codes shore height, the other the level of exposure to solar radiation)
* 3 characters for replicate number (2 digits for replicate number, 1 character for device replacements)

In the example above we have four loggers from site "nozzz" (no = Norway + zzz = site ZZZ), all installed at the mid-shore level (m), half in areas of the shore with maximum local solar exposure (h = hot; we don't use north/south because that doesn't work across hemispheres) and the other half in crevices and other shaded areas (c = cold). There are 2 replicates in each microhabitat.

When the naming scheme used matches the CCTBON scheme, using the function `parse_id_cctbon()` generates new columns coding for site, level, exposure and replicate number. These will be important for subsequent analyses of the dataset that rely on the grouping of data based on those attributes.

In alternative, if the naming scheme is different, `parse_id()` provides a less immediate but more flexible way to parse logger ids.
```{r cctbon1}
# when logger ids follow a different naming scheme, ids can still be parsed
# note the new columns site, mic and rep
# note as well that parse_id acccepts "env" as a list or as a tibble (env$report), 
#  but, in both cases, the output consists only of env$report
parse_id(
  env, 
  div = "1111122333", 
  fields = list(
    site = "fct", 
    mic = c("mc", "mh"), 
    rep = "chr"
    )
  )
```

```{r cctbon2}
# however, when logger ids conform to the CCTBON naming scheme, parsing ids is much simpler
# note the new columns site, lvl, exp, rep1 and rep2
(env <- parse_id_cctbon(env))
```

## Adding other bits of information

If there are additional bits of information that may be useful for subsequent analyses, those can be added to the dataset using `add_info()`. A typical case is the appending of latlon coordinates. `add_info()` requires the presence of column names `site`, as information is added by site.
```{r add_info}
env <- add_info(
   env,
   info = data.frame(
       treatment = 1,
       site = "nozzz",
       lat = 65,
       lon = 10
   )
  )
env[, c("id", "site", "lat", "lon", "treatment")]
```

## Inspecting data

The **envlogger** package has built-in features that ensure that data can be confidently imported and joined together by logger serial and id. However, it is always good practice to inspect the data and look for loggers that may have malfunctioned in a way that cannot be detected automatically (e.g., readings have drifted but remain within a valid range) or for inconsistencies at the joining points. To make that process simpler, call `plot_env()` with `dy = TRUE`, as that will produce an interactive dygraphs plot that can be zoomed and panned. Also, make sure to use the default `col_by = "sgmnt"`. This will ensure that data collected by the same logger (i.e., same serial) that is contained in different files get colored differently, facilitating the identification and inspection of joining points. After inspecting the data and confirming that no issues are present, you can confidently proceed with data analysis.

```{r plot1a}
env <- read_env(
  env_example("ptzzy"), 
  show_progress = FALSE, 
  show_warnings = FALSE)
env <- tidyr::unnest(env$report, data)
env <- dplyr::filter(env, dplyr::between(t, 
  as.POSIXct("2024-01-11", tz = "UTC"), 
  as.POSIXct("2024-01-14", tz = "UTC"))
  )
```
```{r plot1b, eval = FALSE}
plot_env(env, dy = TRUE)
# notice how ids ptzzymh01a and ptzzymc01a are represented by 
#  one green segment until Jan 12, and a purple segment afterwards 
#  (on Jan 12 data was downloaded and the logger memories erased, 
#  while data from that date onwards was downloaded at a later moment, 
#  leading to the entire dataset being contained in two separate files)
```
```{r plot1c, out.width = "750px", echo = FALSE, fig.align = "left"}
knitr::include_graphics("man/figures/README-dyplot-printscreen.png")
```

## Summarising and Visualizing

The main goal of the **envlogger** package is to facilitate the importing and curating of large sets of EnvLogger data. Nevertheless, additional tools are provided for quickly summarising and visualizing large datasets. This is accomplished using the `summarise_env()` function and the ggplot2 options of `plot_env()`. 

```{r plot2}
# summarising often requires data at the exact same timestamps
# to have that, we can provide a value for new_interval, which 
#  will result in all data being interpolated to that new time interval
env <- read_env(
  env_example(c("ptzzw", "ptzzy", "nozzz")), 
  new_interval  = 60,
  show_progress = FALSE, 
  show_warnings = FALSE)$report

env <- parse_id_cctbon(env)

# compute daily q10, mean and q90 for each site
env_summarised <- summarise_env(
  env, 
  by_day = TRUE, 
  by_site = TRUE, 
  fun_list = list(
    ymin = function(x) quantile(x, 0.1),
    temp = function(x) mean(x),
    ymax = function(x) quantile(x, 0.9)
      )
  )

plot_env(env_summarised, gg_ribbon = TRUE, col_by = "site")
```

```{r plot3}
# apply a rolling monthly average to each stat
env_roll <- summarise_env(
  env, 
  by_day = TRUE, 
  by_site = TRUE, 
  fun_list = list(
    ymin = function(x) quantile(x, 0.1),
    temp = function(x) mean(x),
    ymax = function(x) quantile(x, 0.9)
      ),
  roll_days = 30
  )

plot_env(env_roll, gg_ribbon = TRUE, col_by = "site")
```
