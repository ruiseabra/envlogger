% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tools.R
\name{create_metadata_file}
\alias{create_metadata_file}
\title{Create a metadata file}
\usage{
create_metadata_file(paths, new_vals = NULL, update = FALSE)
}
\arguments{
\item{new_vals}{A list with values to be added to the metadata file about to be created. If not supplied, a default template file with all fields set to \code{NA} is generated. Elements in \code{new_vals} must have the exact same name of the field they correspond to, and the values provided must be single element vectors convertible to character. Elements with names not present in the \code{field} column of the template metadata file are silently disregarded (additional details below).}

\item{update}{If a metadata file is already present, should the execution stop with an error or should the file be updated with the new values (new values take precedence over the values in the original file, but values in the previous file for fields not covered in the current \code{new_vals} are left unchanged).}

\item{path}{The path to a single data report file (additional details below).}
}
\value{
The path of the metadata file that has been created. In addition, an EnvLogger metadata file is created. The file path to the new file is generated by appending \verb{_meta.csv} to the end of the \code{path} provided. The file created is a \code{CSV} file with four columns:
\itemize{
\item \code{field} - the name of the metadata field
\item \code{new_val} - the new value to be assigned
\item \code{type} - the format to which \code{new_val} will be converted
\item \code{info} - details about the meaning and constraints of each \code{field}
}
}
\description{
Create an \code{EnvLogger} metadata file. These files can be used to provide additional information about the matching data file, as well as to store correction details necessary for the successful import of data using \code{\link[=read_env]{read_env()}}.
}
\section{Why use metadata files}{

Large databases of environmental data often include files with errors, inconsistencies and formatting issues. While users can correct such files manually and definitively, such an approach removes the ability to traceback the changes made, sometimes making it impossible to revert those changes later on. This can be a source of many headaches, especially when data has been collected by multiple individuals, using different devices and from distinct locations - and it only gets tougher the larger the data collection network. One should never underestimate the likelihood of new information coming to light years later that implies a reorganization of the database, in which case reverting some of the changes made earlier may be necessary. A way to mitigate this effect is to store corrections in a different file. In this approach, the original file remains intact, and it is the job of the matching metadata file to store the changes and corrections that need to be applied in order for the original data to make perfect sense within the database. Inspecting the metadata file is quick and changes introduced are explicit, traceable and easily revertible. Furthermore, since the metadata file only contains essential fields, the disk space metadata files take up is negligeble. That is why the \link{envlogger-package} provides tools to create and read metadata files, and why our team advocates for the use of such files instead of implementing changes/corrections permanently into the original files.
}

\section{Corrections to EnvLogger files}{

While EnvLogger report files should have no formatting issues, the data they contain may still require corrections. In this case, there are two use scenarios. In the first, the user is aware of a specific issue and wants to pro-actively address it. To do that, the user can generate a template metadata file (if the \code{new_vals} argument isn't provided, all fields are set to \code{NA}) and then manually edit the relevant fields. As long as the metadata file is kept in the same folder in which the associated file is stored, the changes it encodes will be reflected when \code{\link[=read_env]{read_env()}} is used to import the data. The second situation corresponds to when \code{\link[=read_env]{read_env()}} itself identifies certain 'standard' issues for which it can provide a correction. In that case, if the user allows, \code{\link[=read_env]{read_env()}} can automatically generate metadata files that will already include the correction parameters necessary. For example, if \code{\link[=read_env]{read_env()}} detects that a file includes timestamps obviously wrong (such as '1970-01-01 00:00'), it will generate a metadata file where the field \code{purge_time_bef} is set to '2000-01-01 00:00'. This way, the next time \code{\link[=read_env]{read_env()}} is executed, all entries in the offending file with a timestamp before '2000-01-01 00:00' will be discarded, and data import can resume without issue. Note that an argument could be made that the correction could just be silently applied without the need to generate a metadata file, making the process simpler and reducing clutter. However, such an approach would leave the user blindsided, unaware of changes that may introduce biases in the dataset. Since environmental datasets are often used for high-stakes research, conservation and management, such a level of ambiguity is unacceptable, and therefore the more explicit and traceable approach is used instead.
}

\section{Data files not conforming to the EnvLogger ecosystem}{

The \link{envlogger-package} package is built around the EnvLogger ecosystem - devices, smartphone apps and the files they generate. However, our team is aware that many users have data generated using other devices that they want to easily integrate into their environmental datasets. While not all data formats can be accommodated, some flexibility can be gained through the use of metadata files. In this case, these files are used to pass on to \code{\link[=read_env]{read_env()}} certain basic bits of information essential for their correct reading, namely how many lines of header to skip, the format used to store time and a few other parameters. To this effect, there is no automation provided, and the user must generate one metadata file at a time for each of the non-conforming files and manually edit the relevant fields. While this can be laborious, it only needs to be done once for each file. Afterwards, the files are read normally as if the were standard EnvLogger files.
}

\section{Supported files}{

\itemize{
\item EnvLogger data report files with data (not just the header)
\item EnvLogger log files
\item EnvLogger metadata files (created with \code{\link[=create_metadata_file]{create_metadata_file()}})
\item Other data report files originating from devices other than EnvLoggers, as long as they are accompanied by an EnvLogger metadata file with values set for fields required for their correct interpretation
}
}

\section{Non-EnvLogger data report files}{

To ensure a high likelihood of success reading data generated outside of the EnvLogger ecosystem using \code{\link[=read_env]{read_env()}}, not only must each non-Envlogger data report file be accompanied by a metadata file created with \code{\link[=create_metadata_file]{create_metadata_file()}}, they must also obey certain simple structure rules, namely:
\itemize{
\item be a text file, typically \code{CSV} or \code{TXT}
\item if present, any contextual information must all be at the head of the file
\item data must be stored by rows
\item the data section may include column names, but that isn't mandatory
\item each row of data must be in the form \verb{date_time, temperature}
\item each row of data must include only two columns
\item temperature must be represented only as a number (not including the unit)
\item the last line of the file is the last row of data
Then, certain information must be provided via the metadata files, namely:
\item skip = number of lines to skip to get to the first row of actual data (not the column names, if present; can be zero or any number of lines)
\item time_format = the format of date and time (not needed if date_time is in the standard format \verb{2025-01-01 10:30:00}, corresponding to "\%Y-\%m-\%d \%H:\%M:\%S; check \code{\link[=strptime]{strptime()}} for more details about time formats)
\item sep_dec_comma = set to \code{TRUE} if the temperature decimal separator is a comma (not needed if the decimal sparator is the standard dot \code{"."})
\item additional fields such as \code{id}, \code{serial}, \code{tdiff} and others may be needed to ensure that all features available in \code{\link[=read_env]{read_env()}} are leveraged, but importing can proceed without them
}
}

\section{Issues - Unsupported files}{

\itemize{
\item Any file not listed in the section \strong{Supported files}.
\item These files can be left in place without interfering with data import.
\item If you want to silence this warning, either remove affected files from your database or add their paths to the \code{avoid_pattern} arg when calling \code{\link[=read_env]{read_env()}} (such paths are available after a call to \code{\link[=read_env]{read_env()}} using \verb{$files_with_issues}).
}
}

\section{Issues - Quality issues}{

\itemize{
\item Data report files that have not passed the quality checks.
\item Issues flagged include the presence of temperature values outside the limits provided, timestamps outside the limits provided, \code{NA}s on data or timestamps, or timestamps not unidirectional, constant and without gaps.
\item Files that do not pass quality checks are not included in the output.
\item Therefore, action is needed to ensure that the affected files are imported.
\item To fix this, either manually edit the files addressing the issues identified or (preferably) generate a metadata file with \code{\link[=create_metadata_file]{create_metadata_file()}} for each affected file and edit that metadata file to include the necessary information to resolve the issues. Afterwards, call \code{\link[=read_env]{read_env()}} again to implement the fixes.
\item Alternatively, set the \code{auto_generate_fixes} and \code{auto_implement_fixes} args in \code{\link[=read_env]{read_env()}} to \code{TRUE}, as this will allow for the automatic creation of metadata files addressing the issues identified and the immediate implementation of those fixes.
}
}

\section{Issues - Full overlaps}{

\itemize{
\item Files that fully overlap other files are redundant.
\item Affected files can be left in place without interfering with data import.
\item However, if you want to silence this warning, either remove affected files from your database or add their paths to the \code{avoid_pattern} arg when calling \code{\link[=read_env]{read_env()}}.
}
}

\section{Issues - Partial overlaps SERIALs}{

\itemize{
\item Files listed overlap other files for the same serial by more than the stipulated \code{overlap_max_mins} arg provided in the call to \code{\link[=read_env]{read_env()}}.
\item Affected files are still joined by serial - all data in older files is kept, while overlapping data in newer files is discarded.
\item These overlaps are often benign, but consider double checking.
\item To address this, either review the affected files and manually correct the overlapping issues permanently or generate a metadata file with \code{\link[=create_metadata_file]{create_metadata_file()}} and adjust \code{purge_time_bef}.
}
}

\section{Issues - Partial overlaps IDs}{

\itemize{
\item The typical origin of this issue is linked to a change in the logger device used at a monitoring site.
\item Affected data is still joined by id - all data in older serial is kept, while overlapping data in newer serial is discarded.
\item These overlaps are often benign, but consider double checking.
\item To address this, either review the affected files and manually correct the overlapping issues permanently or generate a metadata file with \code{\link[=create_metadata_file]{create_metadata_file()}} and adjust \code{purge_time_bef}.
\item ATTENTION! In certain situations, set a value for \code{purge_time_aft} instead.
}
}

\section{Real deployment date}{

It is common practice among certain users to start a logger's mission before actually heading to the field and installing the logger. Depending how this is done, such practice can result in data being collected before it is actually meaningful. In other circumstances, if a logger has been deployed in a non-permanent way, the user may have recovered the logger, brought it back to the lab and only then downloaded the data. In this second situation, the logger's memory will include readings collected after the real deployment that are also not meaningful for any analysis.
To exclude such excess data, crate a metadata file and set the following parameters :
\itemize{
\item set \code{purge_time_bef} to the date when the affected logger was actually installed; any data before this date will be discarded by \code{\link[=read_env]{read_env()}}
\item set \code{purge_time_aft} to the date when the affected logger was actually recovered; any data after this date will be discarded by \code{\link[=read_env]{read_env()}}
}
}

\section{Timezone issues}{

Potential situations:
\itemize{
\item The smartphone used to program the loggers was set to the wrong timezone
\item The smartphone used to retrieve the data was set to the wrong timezone
}

Consequences:
\itemize{
\item Data timestamps may be shifted from UTC
\item Time diff may include timezone diff
}

Edit fields:
\itemize{
\item \code{offset_time} = shift timestamps by \code{x} seconds (pos or neg)
\item \code{offset_diff} = shift time diff by \code{x} seconds (pos or neg)
}
}

\examples{
path <- env_example("2024-01-12/ptzzymh02a-04CB_CC00_1507_0C-20240112_083030.csv")$rep
new_vals = list(
   id = "test",
   tdiff = 10,
   purge_temp_min = -60,
   split_time_gap = TRUE,
   bad_field = "THIS WILL NOT SHOW UP IN THE FILE GENERATED")

# create just an empty metadata file template
fn <- create_metadata_file(path, overwrite = TRUE)
readLines(fn)
read.csv(fn, skip = 4)$new_val # all fields still set to NA

# create a metadata file with some values already filled in
fn <- create_metadata_file(path, new_vals, overwrite = TRUE)
read.csv(fn, skip = 4)$new_val # some of the fields now feature specific values
}
